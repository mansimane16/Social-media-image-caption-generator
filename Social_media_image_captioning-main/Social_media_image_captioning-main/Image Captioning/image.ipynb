{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:34:14.990195Z",
     "iopub.status.busy": "2022-03-06T06:34:14.989864Z",
     "iopub.status.idle": "2022-03-06T06:34:19.970661Z",
     "shell.execute_reply": "2022-03-06T06:34:19.969956Z",
     "shell.execute_reply.started": "2022-03-06T06:34:14.990114Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:34:19.972539Z",
     "iopub.status.busy": "2022-03-06T06:34:19.972303Z",
     "iopub.status.idle": "2022-03-06T06:34:19.978812Z",
     "shell.execute_reply": "2022-03-06T06:34:19.977587Z",
     "shell.execute_reply.started": "2022-03-06T06:34:19.972501Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = r'C:\\Users\\Acer\\Image Captioning\\data set\\instagram_data'\n",
    "WORKING_DIR =  r'C:\\Users\\Acer\\Image Captioning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:34:19.982304Z",
     "iopub.status.busy": "2022-03-06T06:34:19.982057Z",
     "iopub.status.idle": "2022-03-06T06:34:26.712594Z",
     "shell.execute_reply": "2022-03-06T06:34:26.711913Z",
     "shell.execute_reply.started": "2022-03-06T06:34:19.982271Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134260544 (512.16 MB)\n",
      "Trainable params: 134260544 (512.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "\n",
    "# Load VGG16 model\n",
    "model = VGG16()\n",
    "\n",
    "# Restructure the model\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "# summarize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:34:26.714171Z",
     "iopub.status.busy": "2022-03-06T06:34:26.713924Z",
     "iopub.status.idle": "2022-03-06T06:42:34.816462Z",
     "shell.execute_reply": "2022-03-06T06:42:34.815809Z",
     "shell.execute_reply.started": "2022-03-06T06:34:26.714138Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                          | 157/34927 [02:51<13:49:26,  1.43s/it]"
     ]
    }
   ],
   "source": [
    "# extract features from image\n",
    "\n",
    "features = {}\n",
    "directory = os.path.join(BASE_DIR,'img')\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    # load the image from file\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocess image for vgg\n",
    "    image = preprocess_input(image)\n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    # get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # store feature\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:34.819143Z",
     "iopub.status.busy": "2022-03-06T06:42:34.818823Z",
     "iopub.status.idle": "2022-03-06T06:42:35.148181Z",
     "shell.execute_reply": "2022-03-06T06:42:35.147422Z",
     "shell.execute_reply.started": "2022-03-06T06:42:34.819101Z"
    }
   },
   "outputs": [],
   "source": [
    "# store features in pickle\n",
    "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features_insta.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.150092Z",
     "iopub.status.busy": "2022-03-06T06:42:35.149415Z",
     "iopub.status.idle": "2022-03-06T06:42:35.312615Z",
     "shell.execute_reply": "2022-03-06T06:42:35.311884Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.150052Z"
    }
   },
   "outputs": [],
   "source": [
    "# load features from pickle\n",
    "with open(os.path.join(WORKING_DIR, 'features3.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Captions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.315520Z",
     "iopub.status.busy": "2022-03-06T06:42:35.314779Z",
     "iopub.status.idle": "2022-03-06T06:42:35.378135Z",
     "shell.execute_reply": "2022-03-06T06:42:35.377462Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.315478Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.379822Z",
     "iopub.status.busy": "2022-03-06T06:42:35.379546Z",
     "iopub.status.idle": "2022-03-06T06:42:35.520768Z",
     "shell.execute_reply": "2022-03-06T06:42:35.520076Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.379783Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 40456/40456 [00:00<00:00, 377010.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# create mapping of image to captions\n",
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # convert caption list to string\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.522414Z",
     "iopub.status.busy": "2022-03-06T06:42:35.522031Z",
     "iopub.status.idle": "2022-03-06T06:42:35.530158Z",
     "shell.execute_reply": "2022-03-06T06:42:35.529170Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.522377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.531789Z",
     "iopub.status.busy": "2022-03-06T06:42:35.531417Z",
     "iopub.status.idle": "2022-03-06T06:42:35.539936Z",
     "shell.execute_reply": "2022-03-06T06:42:35.539143Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.531753Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # delete digits, special chars, etc., \n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.541972Z",
     "iopub.status.busy": "2022-03-06T06:42:35.541482Z",
     "iopub.status.idle": "2022-03-06T06:42:35.550972Z",
     "shell.execute_reply": "2022-03-06T06:42:35.549978Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.541937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before preprocess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.553409Z",
     "iopub.status.busy": "2022-03-06T06:42:35.552825Z",
     "iopub.status.idle": "2022-03-06T06:42:35.692145Z",
     "shell.execute_reply": "2022-03-06T06:42:35.691520Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.553368Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.693654Z",
     "iopub.status.busy": "2022-03-06T06:42:35.693412Z",
     "iopub.status.idle": "2022-03-06T06:42:35.698691Z",
     "shell.execute_reply": "2022-03-06T06:42:35.697951Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.693623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       " 'startseq girl going into wooden building endseq',\n",
       " 'startseq little girl climbing into wooden playhouse endseq',\n",
       " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq little girl in pink dress going into wooden cabin endseq']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after preprocess of text\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.703681Z",
     "iopub.status.busy": "2022-03-06T06:42:35.703176Z",
     "iopub.status.idle": "2022-03-06T06:42:35.716917Z",
     "shell.execute_reply": "2022-03-06T06:42:35.716311Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.703644Z"
    }
   },
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.719878Z",
     "iopub.status.busy": "2022-03-06T06:42:35.719667Z",
     "iopub.status.idle": "2022-03-06T06:42:35.728236Z",
     "shell.execute_reply": "2022-03-06T06:42:35.727288Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.719843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.730327Z",
     "iopub.status.busy": "2022-03-06T06:42:35.730058Z",
     "iopub.status.idle": "2022-03-06T06:42:35.737755Z",
     "shell.execute_reply": "2022-03-06T06:42:35.736976Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.730295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       " 'startseq girl going into wooden building endseq',\n",
       " 'startseq little girl climbing into wooden playhouse endseq',\n",
       " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq little girl in pink dress going into wooden cabin endseq',\n",
       " 'startseq black dog and spotted dog are fighting endseq',\n",
       " 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n",
       " 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n",
       " 'startseq two dogs of different breeds looking at each other on the road endseq',\n",
       " 'startseq two dogs on pavement moving toward each other endseq']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:35.739724Z",
     "iopub.status.busy": "2022-03-06T06:42:35.739327Z",
     "iopub.status.idle": "2022-03-06T06:42:36.355776Z",
     "shell.execute_reply": "2022-03-06T06:42:36.355062Z",
     "shell.execute_reply.started": "2022-03-06T06:42:35.739686Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:36.357418Z",
     "iopub.status.busy": "2022-03-06T06:42:36.357182Z",
     "iopub.status.idle": "2022-03-06T06:42:36.362429Z",
     "shell.execute_reply": "2022-03-06T06:42:36.361793Z",
     "shell.execute_reply.started": "2022-03-06T06:42:36.357385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8485"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:36.364347Z",
     "iopub.status.busy": "2022-03-06T06:42:36.363844Z",
     "iopub.status.idle": "2022-03-06T06:42:36.403655Z",
     "shell.execute_reply": "2022-03-06T06:42:36.402785Z",
     "shell.execute_reply.started": "2022-03-06T06:42:36.364309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:36.406025Z",
     "iopub.status.busy": "2022-03-06T06:42:36.405019Z",
     "iopub.status.idle": "2022-03-06T06:42:36.411229Z",
     "shell.execute_reply": "2022-03-06T06:42:36.410251Z",
     "shell.execute_reply.started": "2022-03-06T06:42:36.405987Z"
    }
   },
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:36.413094Z",
     "iopub.status.busy": "2022-03-06T06:42:36.412818Z",
     "iopub.status.idle": "2022-03-06T06:42:36.420669Z",
     "shell.execute_reply": "2022-03-06T06:42:36.419873Z",
     "shell.execute_reply.started": "2022-03-06T06:42:36.413060Z"
    }
   },
   "outputs": [],
   "source": [
    "# startseq girl going into wooden building endseq\n",
    "#        X                   y\n",
    "# startseq                   girl\n",
    "# startseq girl              going\n",
    "# startseq girl going        into\n",
    "# ...........\n",
    "# startseq girl going into wooden building      endseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:36.422292Z",
     "iopub.status.busy": "2022-03-06T06:42:36.422033Z",
     "iopub.status.idle": "2022-03-06T06:42:36.434535Z",
     "shell.execute_reply": "2022-03-06T06:42:36.433711Z",
     "shell.execute_reply.started": "2022-03-06T06:42:36.422261Z"
    }
   },
   "outputs": [],
   "source": [
    "# create data generator to get data in batch (avoids session crash)\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    # store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:36.435949Z",
     "iopub.status.busy": "2022-03-06T06:42:36.435684Z",
     "iopub.status.idle": "2022-03-06T06:42:38.472355Z",
     "shell.execute_reply": "2022-03-06T06:42:38.469216Z",
     "shell.execute_reply.started": "2022-03-06T06:42:36.435912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# encoder model\n",
    "# image feature layers\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T06:42:38.474530Z",
     "iopub.status.busy": "2022-03-06T06:42:38.474040Z",
     "iopub.status.idle": "2022-03-06T07:06:15.926491Z",
     "shell.execute_reply": "2022-03-06T07:06:15.925776Z",
     "shell.execute_reply.started": "2022-03-06T06:42:38.474487Z"
    }
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    # create data generator\n",
    "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T07:06:15.928220Z",
     "iopub.status.busy": "2022-03-06T07:06:15.927965Z",
     "iopub.status.idle": "2022-03-06T07:06:16.090583Z",
     "shell.execute_reply": "2022-03-06T07:06:16.088709Z",
     "shell.execute_reply.started": "2022-03-06T07:06:15.928186Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(WORKING_DIR+'/best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Captions for the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T07:06:16.093448Z",
     "iopub.status.busy": "2022-03-06T07:06:16.092736Z",
     "iopub.status.idle": "2022-03-06T07:06:16.100031Z",
     "shell.execute_reply": "2022-03-06T07:06:16.098267Z",
     "shell.execute_reply.started": "2022-03-06T07:06:16.093377Z"
    }
   },
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T07:06:16.108028Z",
     "iopub.status.busy": "2022-03-06T07:06:16.102398Z",
     "iopub.status.idle": "2022-03-06T07:06:16.122532Z",
     "shell.execute_reply": "2022-03-06T07:06:16.121669Z",
     "shell.execute_reply.started": "2022-03-06T07:06:16.107982Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "      \n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T07:06:16.124177Z",
     "iopub.status.busy": "2022-03-06T07:06:16.123749Z",
     "iopub.status.idle": "2022-03-06T07:12:27.075826Z",
     "shell.execute_reply": "2022-03-06T07:12:27.075089Z",
     "shell.execute_reply.started": "2022-03-06T07:06:16.124139Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length) \n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "    \n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T07:12:27.077550Z",
     "iopub.status.busy": "2022-03-06T07:12:27.077132Z",
     "iopub.status.idle": "2022-03-06T07:12:27.084620Z",
     "shell.execute_reply": "2022-03-06T07:12:27.083943Z",
     "shell.execute_reply.started": "2022-03-06T07:12:27.077509Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_caption(image_name):\n",
    "    # load the image\n",
    "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T07:12:27.086378Z",
     "iopub.status.busy": "2022-03-06T07:12:27.085839Z",
     "iopub.status.idle": "2022-03-06T07:12:27.811836Z",
     "shell.execute_reply": "2022-03-06T07:12:27.811100Z",
     "shell.execute_reply.started": "2022-03-06T07:12:27.086340Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_caption(\"1001773457_577c3a7d70.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T07:12:27.813391Z",
     "iopub.status.busy": "2022-03-06T07:12:27.812965Z",
     "iopub.status.idle": "2022-03-06T07:12:28.645372Z",
     "shell.execute_reply": "2022-03-06T07:12:28.644695Z",
     "shell.execute_reply.started": "2022-03-06T07:12:27.813352Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_caption(\"1002674143_1b742ab4b8.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-06T07:12:28.647257Z",
     "iopub.status.busy": "2022-03-06T07:12:28.646519Z",
     "iopub.status.idle": "2022-03-06T07:12:29.247604Z",
     "shell.execute_reply": "2022-03-06T07:12:29.246938Z",
     "shell.execute_reply.started": "2022-03-06T07:12:28.647219Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_caption(\"101669240_b2d3e7f17b.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Real Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T10:52:49.644967Z",
     "iopub.status.busy": "2023-02-07T10:52:49.644329Z",
     "iopub.status.idle": "2023-02-07T10:52:52.452913Z",
     "shell.execute_reply": "2023-02-07T10:52:52.452096Z",
     "shell.execute_reply.started": "2023-02-07T10:52:49.644930Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_model = VGG16()\n",
    "# restructure the model\n",
    "vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T10:53:08.419042Z",
     "iopub.status.busy": "2023-02-07T10:53:08.418775Z",
     "iopub.status.idle": "2023-02-07T10:53:09.313035Z",
     "shell.execute_reply": "2023-02-07T10:53:09.312141Z",
     "shell.execute_reply.started": "2023-02-07T10:53:08.419012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startseq the girl is standing on the couch and is standing on the side of the playhouse endseq'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = '/kaggle/input/flickr8k/Images/1000268201_693b08cb0e.jpg'\n",
    "# load image\n",
    "image = load_img(image_path, target_size=(224, 224))\n",
    "# convert image pixels to numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# preprocess image for vgg\n",
    "image = preprocess_input(image)\n",
    "# extract features\n",
    "feature = vgg_model.predict(image, verbose=0)\n",
    "# predict from the trained model\n",
    "predict_caption(model, feature, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image and resize it to match the input size of the model\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((224, 224))  # Adjust the size as per your model's input shape\n",
    "    img = np.array(img)\n",
    "    # Perform any additional preprocessing steps required by your model\n",
    "    return img\n",
    "\n",
    "def generate_caption(model, image_features, tokenizer, max_length):\n",
    "    generated_caption = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        # Tokenize the current generated caption\n",
    "        sequence = tokenizer.texts_to_sequences([generated_caption])[0]\n",
    "        # Pad the sequence to the maximum length\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # Predict next word\n",
    "        predictions = model.predict([image_features, sequence], verbose=0)\n",
    "        # Get the index of the word with the highest probability\n",
    "        predicted_index = np.argmax(predictions)\n",
    "        # Map the index to the corresponding word\n",
    "        predicted_word = idx_to_word(predicted_index, tokenizer)\n",
    "        if predicted_word is None or predicted_word == 'endseq':\n",
    "            break\n",
    "        # Append the predicted word to the generated caption\n",
    "        generated_caption += \" \" + predicted_word\n",
    "    return generated_caption\n",
    "\n",
    "def predict_caption(model, image_path, tokenizer, max_length):\n",
    "    # Preprocess the image\n",
    "    image = preprocess_image(image_path)\n",
    "    # Extract features from the image using a pre-trained model\n",
    "    image_features = vgg_model.predict(image.reshape(1, 224, 224, 3), verbose=0) # Implement this function according to your feature extraction model\n",
    "    # Generate caption\n",
    "    generated_caption = generate_caption(model, image_features, tokenizer, max_length)\n",
    "    return generated_caption\n",
    "\n",
    "# Example usage:\n",
    "image_path = r'C:\\Users\\ARYAN\\Downloads\\96740a291e22e246507d6df3f1f34128.jpg'\n",
    "predicted_caption = predict_caption(custom_model, image_path, tokenizer, max_length)\n",
    "print(\"Predicted Caption:\", predicted_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
